name: Compute history

# History of Premier partners
# https://partners.amazonaws.com/partners/001E000000Ufa9FIAR/Thoughtworks

on:
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repo
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
      - uses: actions/setup-python@v4
        with:
          python-version: "3.11"
      - run: pip install -r requirements.txt
      - run: datasette plugins
      - name: Compute history
        run: |-
          git log --pretty=format:"%ad %H" --date=short | while read -r date hash
          do
                echo $date $hash
                git checkout $hash -- partners/ || break
                rm partners.db || true
                sqlite-diffable load partners.db partners
                sqlite3 -csv partners.db "select '${date}' AS 'date', literal_name, aws_certifications_count from partners where current_program_status = 'Premier' and customer_type = 'Consulting Partner';" >> premier.csv
          done
          wc -l premier.csv
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::407461997746:role/github-actions-Role-56IHHM969DKJ
          aws-region: eu-west-2
      - name: Upload to S3
        run: |-
          bucket_name="aws-partners-datasette"
          region="eu-west-2"
          aws s3 mb s3://$bucket_name --region $region || true
          aws s3 cp premier.csv s3://$bucket_name/
          aws s3 presign s3://$bucket_name/premier.csv
